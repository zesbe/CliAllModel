#!/bin/bash

# Colors
GREEN='\033[0;32m'
BLUE='\033[0;34m'
RED='\033[0;31m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

LITELLM_PORT=8555
LITELLM_HOST="http://127.0.0.1:$LITELLM_PORT"

check_dependencies() {
    echo -e "${BLUE}Checking dependencies...${NC}"
    
    if ! command -v npm &> /dev/null; then
        echo -e "${RED}Error: npm is not installed.${NC}"
        exit 1
    fi

    if ! command -v claude &> /dev/null; then
        echo -e "${YELLOW}'claude' command not found. Installing @anthropic-ai/claude-code globally...${NC}"
        npm install -g @anthropic-ai/claude-code
    fi

    if ! command -v python3 &> /dev/null; then
        echo -e "${RED}Error: python3 is not installed.${NC}"
        exit 1
    fi

    if ! python3 -m pip show litellm &> /dev/null; then
        echo -e "${YELLOW}'litellm' not found. Installing via pip...${NC}"
        python3 -m pip install litellm[proxy]
    fi
}

check_gcloud() {
    if ! command -v gcloud &> /dev/null; then
        echo -e "${RED}Error: Google Cloud CLI (gcloud) is required for OAuth login.${NC}"
        echo "Please install it: https://cloud.google.com/sdk/docs/install"
        exit 1
    fi
}

start_litellm_proxy() {
    local model=$1
    echo -e "${BLUE}Starting LiteLLM proxy for model: $model...${NC}"
    
    # Kill any existing litellm on this port
    fuser -k $LITELLM_PORT/tcp &> /dev/null || true

    # Start litellm in background
    # --drop_params prevents Anthropic-specific params from breaking other providers
    python3 -m litellm --model "$model" --port $LITELLM_PORT --drop_params &> /tmp/litellm.log &
    LITELLM_PID=$!
    
    # Wait for it to start
    echo -n "Waiting for proxy to start..."
    for i in {1..10}; do
        if curl -s $LITELLM_HOST/health &> /dev/null; then
            echo -e " ${GREEN}Ready!${NC}"
            return 0
        fi
        sleep 1
        echo -n "."
    done
    
    echo -e "\n${RED}Failed to start LiteLLM proxy. Check logs:${NC}"
    cat /tmp/litellm.log
    kill $LITELLM_PID 2>/dev/null
    exit 1
}

cleanup() {
    if [ ! -z "$LITELLM_PID" ]; then
        echo -e "\n${BLUE}Stopping LiteLLM proxy...${NC}"
        kill $LITELLM_PID 2>/dev/null
    fi
}

trap cleanup EXIT

# Main Menu
clear
echo -e "${GREEN}=====================================${NC}"
echo -e "${GREEN}    Claude Code Multi-Model Launcher ${NC}"
echo -e "${GREEN}=====================================${NC}"
echo "Select your AI Provider:"
echo "1) MiniMax (Direct Anthropic API)"
echo "2) Google Gemini (API Key - AI Studio)"
echo "3) Google Gemini (OAuth - Vertex AI)"
echo "4) OpenAI (API Key)"
echo "5) xAI / Grok (API Key)"
echo "6) ZhipuAI / GLM (API Key)"
echo "7) Groq (API Key)"
echo "8) Custom / Other"
echo -e "${GREEN}=====================================${NC}"
read -p "Enter choice [1-8]: " choice

export ANTHROPIC_MODEL="claude-3-sonnet-20240229" # Dummy model name for CLI validation

case $choice in
    1)
        # MiniMax Direct
        echo -e "${BLUE}Configuring for MiniMax...${NC}"
        if [ -z "$ANTHROPIC_API_KEY" ]; then
            echo "Get Key: https://platform.minimax.io/"
            read -p "Enter MiniMax API Key: " ANTHROPIC_API_KEY
            export ANTHROPIC_API_KEY
        fi
        export ANTHROPIC_BASE_URL="https://api.minimax.io/anthropic"
        echo -e "${YELLOW}Note: MiniMax maps model names automatically.${NC}"
        exec claude
        ;;
    2)
        # Gemini API Key
        check_dependencies
        echo -e "${BLUE}Configuring for Gemini (AI Studio)...${NC}"
        if [ -z "$GEMINI_API_KEY" ]; then
            echo "Get Key: https://aistudio.google.com/app/apikey"
            read -p "Enter Gemini API Key: " GEMINI_API_KEY
            export GEMINI_API_KEY
        fi
        MODEL_NAME="gemini/gemini-2.0-flash"
        read -p "Enter Model Name [default: $MODEL_NAME]: " input_model
        [ ! -z "$input_model" ] && MODEL_NAME=$input_model
        
        start_litellm_proxy "$MODEL_NAME"
        export ANTHROPIC_BASE_URL="$LITELLM_HOST"
        export ANTHROPIC_API_KEY="sk-litellm"
        claude
        ;;
    3)
        # Gemini Vertex OAuth
        check_dependencies
        check_gcloud
        echo -e "${BLUE}Configuring for Gemini (Vertex AI OAuth)...${NC}"
        
        echo "Checking gcloud authentication..."
        if ! gcloud auth print-access-token &> /dev/null; then
             echo "Redirecting to browser for Login..."
             gcloud auth application-default login
        fi

        if [ -z "$VERTEX_PROJECT" ]; then
            read -p "Enter Google Cloud Project ID: " VERTEX_PROJECT
            export VERTEX_PROJECT
        fi
        
        # Vertex specific env setup for LiteLLM
        export VERTEXAI_PROJECT="$VERTEX_PROJECT"
        export VERTEXAI_LOCATION="us-central1"
        
        MODEL_NAME="vertex_ai/gemini-1.5-pro"
        read -p "Enter Model Name [default: $MODEL_NAME]: " input_model
        [ ! -z "$input_model" ] && MODEL_NAME=$input_model
        
        start_litellm_proxy "$MODEL_NAME"
        export ANTHROPIC_BASE_URL="$LITELLM_HOST"
        export ANTHROPIC_API_KEY="sk-litellm"
        claude
        ;;
    4)
        # OpenAI
        check_dependencies
        echo -e "${BLUE}Configuring for OpenAI...${NC}"
        if [ -z "$OPENAI_API_KEY" ]; then
            echo "Get Key: https://platform.openai.com/api-keys"
            read -p "Enter OpenAI API Key: " OPENAI_API_KEY
            export OPENAI_API_KEY
        fi
        MODEL_NAME="gpt-4o"
        read -p "Enter Model Name [default: $MODEL_NAME]: " input_model
        [ ! -z "$input_model" ] && MODEL_NAME=$input_model
        
        start_litellm_proxy "$MODEL_NAME"
        export ANTHROPIC_BASE_URL="$LITELLM_HOST"
        export ANTHROPIC_API_KEY="sk-litellm"
        claude
        ;;
    5)
        # xAI
        check_dependencies
        echo -e "${BLUE}Configuring for xAI (Grok)...${NC}"
        if [ -z "$XAI_API_KEY" ]; then
            echo "Get Key: https://console.x.ai/"
            read -p "Enter xAI API Key: " XAI_API_KEY
            export XAI_API_KEY
        fi
        MODEL_NAME="xai/grok-beta"
        
        start_litellm_proxy "$MODEL_NAME"
        export ANTHROPIC_BASE_URL="$LITELLM_HOST"
        export ANTHROPIC_API_KEY="sk-litellm"
        claude
        ;;
    6)
        # ZhipuAI
        check_dependencies
        echo -e "${BLUE}Configuring for ZhipuAI (GLM)...${NC}"
        if [ -z "$ZHIPUAI_API_KEY" ]; then
            read -p "Enter ZhipuAI API Key: " ZHIPUAI_API_KEY
            export ZHIPUAI_API_KEY
        fi
        MODEL_NAME="zhipu/glm-4"
        
        start_litellm_proxy "$MODEL_NAME"
        export ANTHROPIC_BASE_URL="$LITELLM_HOST"
        export ANTHROPIC_API_KEY="sk-litellm"
        claude
        ;;
    7)
        # Groq
        check_dependencies
        echo -e "${BLUE}Configuring for Groq...${NC}"
        if [ -z "$GROQ_API_KEY" ]; then
            echo "Get Key: https://console.groq.com/keys"
            read -p "Enter Groq API Key: " GROQ_API_KEY
            export GROQ_API_KEY
        fi
        MODEL_NAME="groq/llama3-70b-8192"
        
        start_litellm_proxy "$MODEL_NAME"
        export ANTHROPIC_BASE_URL="$LITELLM_HOST"
        export ANTHROPIC_API_KEY="sk-litellm"
        claude
        ;;
    8)
        # Custom
        check_dependencies
        echo -e "${BLUE}Configuring Custom LiteLLM Model...${NC}"
        echo "Format example: ollama/llama3, azure/gpt-4, etc."
        read -p "Enter LiteLLM Model String: " MODEL_NAME
        
        echo "Set any required env vars (e.g., OLLAMA_API_BASE). Press Enter when done."
        read -p "Press Enter to continue..."

        start_litellm_proxy "$MODEL_NAME"
        export ANTHROPIC_BASE_URL="$LITELLM_HOST"
        export ANTHROPIC_API_KEY="sk-litellm"
        claude
        ;;
    *)
        echo "Invalid choice"
        exit 1
        ;;
esac