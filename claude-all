#!/bin/bash

# Colors
GREEN='\033[0;32m'
BLUE='\033[0;34m'
RED='\033[0;31m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

LITELLM_PORT=8555
LITELLM_HOST="http://127.0.0.1:$LITELLM_PORT"
SCRIPT_DIR="$(dirname "$(realpath "$0")")"

check_dependencies() {
    echo -e "${BLUE}Checking dependencies...${NC}"
    
    if ! command -v npm &> /dev/null; then
        echo -e "${RED}Error: npm is not installed.${NC}"
        exit 1
    fi

    if ! command -v claude &> /dev/null; then
        echo -e "${YELLOW}'claude' command not found. Installing @anthropic-ai/claude-code globally...${NC}"
        npm install -g @anthropic-ai/claude-code
    fi

    if ! command -v python3 &> /dev/null; then
        echo -e "${RED}Error: python3 is not installed.${NC}"
        exit 1
    fi

    if ! python3 -m pip show litellm &> /dev/null; then
        echo -e "${YELLOW}'litellm' not found. Installing via pip...${NC}"
        python3 -m pip install litellm[proxy]
    fi
}

check_gemini_oauth() {
    # Check if we have ADC credentials
    ADC_PATH="$HOME/.config/gcloud/application_default_credentials.json"
    
    if [ -f "$ADC_PATH" ]; then
        return 0
    fi
    
    # If not, check if gcloud is installed
    if command -v gcloud &> /dev/null; then
        echo -e "${BLUE}gcloud found. Attempting login...${NC}"
        gcloud auth application-default login
        return
    fi
    
    # Fallback to custom python script
    echo -e "${YELLOW}gcloud not found. Using lightweight Python Auth helper...${NC}"
    
    # Install dependency
    if ! python3 -m pip show google-auth-oauthlib &> /dev/null; then
         echo "Installing google-auth-oauthlib..."
         python3 -m pip install google-auth-oauthlib
    fi
    
    # Run helper script
    AUTH_SCRIPT="$SCRIPT_DIR/gemini_auth.py"
    if [ ! -f "$AUTH_SCRIPT" ]; then
        curl -fsSL https://raw.githubusercontent.com/zesbe/CliAllModel/main/gemini_auth.py -o "$HOME/.local/bin/gemini_auth.py"
        AUTH_SCRIPT="$HOME/.local/bin/gemini_auth.py"
    fi
    
    python3 "$AUTH_SCRIPT"
    
    if [ ! -f "$ADC_PATH" ]; then
        echo -e "${RED}Authentication failed or cancelled.${NC}"
        exit 1
    fi
}

check_openai_oauth() {
    CRED_PATH="$HOME/.config/openai/credentials.json"
    
    if [ ! -f "$CRED_PATH" ]; then
        echo -e "${YELLOW}No cached OpenAI OAuth token found. Launching helper...${NC}"
        
        # Install dependency
        python3 -m pip install requests > /dev/null 2>&1

        AUTH_SCRIPT="$SCRIPT_DIR/openai_auth.py"
        if [ ! -f "$AUTH_SCRIPT" ]; then
            curl -fsSL https://raw.githubusercontent.com/zesbe/CliAllModel/main/openai_auth.py -o "$HOME/.local/bin/openai_auth.py"
            AUTH_SCRIPT="$HOME/.local/bin/openai_auth.py"
        fi
        
        python3 "$AUTH_SCRIPT"
        
        if [ ! -f "$CRED_PATH" ]; then
             echo -e "${RED}OpenAI OAuth failed.${NC}"
             exit 1
        fi
    fi
    
    # Extract access token
    OPENAI_ACCESS_TOKEN=$(python3 -c "import json, os; print(json.load(open(os.path.expanduser('$CRED_PATH')))['access_token'])")
    export OPENAI_API_KEY="$OPENAI_ACCESS_TOKEN"
}

start_litellm_proxy() {
    local model=$1
    echo -e "${BLUE}Starting LiteLLM proxy for model: $model...${NC}"
    
    # Kill any existing litellm on this port
    fuser -k $LITELLM_PORT/tcp &> /dev/null || true

    # Start litellm in background
    # --drop_params prevents Anthropic-specific params from breaking other providers
    python3 -m litellm --model "$model" --port $LITELLM_PORT --drop_params &> /tmp/litellm.log &
    LITELLM_PID=$!
    
    # Wait for it to start
    echo -n "Waiting for proxy to start..."
    for i in {1..10}; do
        if curl -s $LITELLM_HOST/health &> /dev/null; then
            echo -e " ${GREEN}Ready!${NC}"
            return 0
        fi
        sleep 1
        echo -n "."
    done
    
    echo -e "\n${RED}Failed to start LiteLLM proxy. Check logs:${NC}"
    cat /tmp/litellm.log
    kill $LITELLM_PID 2>/dev/null
    exit 1
}

cleanup() {
    if [ ! -z "$LITELLM_PID" ]; then
        echo -e "\n${BLUE}Stopping LiteLLM proxy...${NC}"
        kill $LITELLM_PID 2>/dev/null
    fi
}

trap cleanup EXIT

# Main Menu
choice=$1

if [ -z "$choice" ]; then
    clear
    echo -e "${GREEN}=====================================${NC}"
    echo -e "${GREEN}    Claude Code Multi-Model Launcher ${NC}"
    echo -e "${GREEN}=====================================${NC}"
    echo "Select your AI Provider:"
    echo "1) MiniMax (Direct Anthropic API)"
    echo "2) Google Gemini (API Key - AI Studio)"
    echo "3) Google Gemini (OAuth - Vertex AI)"
    echo "4) OpenAI (API Key)"
    echo "5) OpenAI (OAuth - Experimental)"
    echo "6) xAI / Grok (API Key)"
    echo "7) ZhipuAI / GLM (API Key)"
    echo "8) Groq (API Key)"
    echo "9) Ollama (Local Models)"
    echo "10) Custom / Other"
    echo -e "${GREEN}=====================================${NC}"
    read -p "Enter choice [1-10]: " choice
fi

export ANTHROPIC_MODEL="claude-3-sonnet-20240229" # Dummy model name for CLI validation

case $choice in
    1)
        # MiniMax Direct
        echo -e "${BLUE}Configuring for MiniMax...${NC}"
        if [ -z "$ANTHROPIC_API_KEY" ]; then
            echo "Get Key: https://platform.minimax.io/"
            read -p "Enter MiniMax API Key: " ANTHROPIC_API_KEY
            export ANTHROPIC_API_KEY
        fi
        export ANTHROPIC_BASE_URL="https://api.minimax.io/anthropic"
        echo -e "${YELLOW}Note: MiniMax maps model names automatically.${NC}"
        exec claude "$@"
        ;;
    2)
        # Gemini API Key
        check_dependencies
        echo -e "${BLUE}Configuring for Gemini (AI Studio)...${NC}"
        if [ -z "$GEMINI_API_KEY" ]; then
            echo "Get Key: https://aistudio.google.com/app/apikey"
            read -p "Enter Gemini API Key: " GEMINI_API_KEY
            export GEMINI_API_KEY
        fi
        MODEL_NAME="gemini/gemini-1.5-flash"
        read -p "Enter Model Name [default: $MODEL_NAME]: " input_model
        [ ! -z "$input_model" ] && MODEL_NAME=$input_model
        
        start_litellm_proxy "$MODEL_NAME"
        export ANTHROPIC_BASE_URL="$LITELLM_HOST"
        export ANTHROPIC_API_KEY="sk-litellm"
        exec claude "$@"
        ;;
    3)
        # Gemini Vertex OAuth
        check_dependencies
        echo -e "${BLUE}Configuring for Gemini (Vertex AI OAuth)...${NC}"
        
        check_gemini_oauth

        if [ -z "$VERTEX_PROJECT" ]; then
            read -p "Enter Google Cloud Project ID: " VERTEX_PROJECT
            export VERTEX_PROJECT
        fi
        
        export VERTEXAI_PROJECT="$VERTEX_PROJECT"
        export VERTEXAI_LOCATION="us-central1"
        
        MODEL_NAME="vertex_ai/gemini-1.5-pro"
        read -p "Enter Model Name [default: $MODEL_NAME]: " input_model
        [ ! -z "$input_model" ] && MODEL_NAME=$input_model
        
        start_litellm_proxy "$MODEL_NAME"
        export ANTHROPIC_BASE_URL="$LITELLM_HOST"
        export ANTHROPIC_API_KEY="sk-litellm"
        exec claude "$@"
        ;;
    4)
        # OpenAI API Key
        check_dependencies
        echo -e "${BLUE}Configuring for OpenAI...${NC}"
        if [ -z "$OPENAI_API_KEY" ]; then
            echo "Get Key: https://platform.openai.com/api-keys"
            read -p "Enter OpenAI API Key: " OPENAI_API_KEY
            export OPENAI_API_KEY
        fi
        MODEL_NAME="gpt-4o"
        read -p "Enter Model Name [default: $MODEL_NAME]: " input_model
        [ ! -z "$input_model" ] && MODEL_NAME=$input_model
        
        start_litellm_proxy "$MODEL_NAME"
        export ANTHROPIC_BASE_URL="$LITELLM_HOST"
        export ANTHROPIC_API_KEY="sk-litellm"
        exec claude "$@"
        ;;
    5)
        # OpenAI OAuth (Experimental)
        check_dependencies
        echo -e "${BLUE}Configuring for OpenAI (OAuth Experimental)...${NC}"
        
        check_openai_oauth
        
        MODEL_NAME="gpt-4o"
        read -p "Enter Model Name [default: $MODEL_NAME]: " input_model
        [ ! -z "$input_model" ] && MODEL_NAME=$input_model
        
        start_litellm_proxy "$MODEL_NAME"
        export ANTHROPIC_BASE_URL="$LITELLM_HOST"
        export ANTHROPIC_API_KEY="sk-litellm"
        exec claude "$@"
        ;;
    6)
        # xAI
        check_dependencies
        echo -e "${BLUE}Configuring for xAI (Grok)...${NC}"
        if [ -z "$XAI_API_KEY" ]; then
            echo "Get Key: https://console.x.ai/"
            read -p "Enter xAI API Key: " XAI_API_KEY
            export XAI_API_KEY
        fi
        MODEL_NAME="xai/grok-1"
        
        start_litellm_proxy "$MODEL_NAME"
        export ANTHROPIC_BASE_URL="$LITELLM_HOST"
        export ANTHROPIC_API_KEY="sk-litellm"
        exec claude "$@"
        ;;
    7)
        # ZhipuAI
        check_dependencies
        echo -e "${BLUE}Configuring for ZhipuAI (GLM)...${NC}"
        if [ -z "$ZHIPUAI_API_KEY" ]; then
            read -p "Enter ZhipuAI API Key: " ZHIPUAI_API_KEY
            export ZHIPUAI_API_KEY
        fi
        MODEL_NAME="zhipu/glm-4"
        
        start_litellm_proxy "$MODEL_NAME"
        export ANTHROPIC_BASE_URL="$LITELLM_HOST"
        export ANTHROPIC_API_KEY="sk-litellm"
        exec claude "$@"
        ;;
    8)
        # Groq
        check_dependencies
        echo -e "${BLUE}Configuring for Groq...${NC}"
        if [ -z "$GROQ_API_KEY" ]; then
            echo "Get Key: https://console.groq.com/keys"
            read -p "Enter Groq API Key: " GROQ_API_KEY
            export GROQ_API_KEY
        fi
        MODEL_NAME="groq/llama3-70b-8192"
        
        start_litellm_proxy "$MODEL_NAME"
        export ANTHROPIC_BASE_URL="$LITELLM_HOST"
        export ANTHROPIC_API_KEY="sk-litellm"
        exec claude "$@"
        ;;
    9)
        # Ollama
        check_dependencies
        echo -e "${BLUE}Configuring for Ollama...${NC}"
        if [ -z "$OLLAMA_HOST" ]; then
            read -p "Enter Ollama Host [default: http://localhost:11434]: " OLLAMA_HOST
            [ -z "$OLLAMA_HOST" ] && OLLAMA_HOST="http://localhost:11434"
            export OLLAMA_HOST
        fi
        MODEL_NAME="ollama/llama3"
        read -p "Enter Model Name [default: $MODEL_NAME]: " input_model
        [ ! -z "$input_model" ] && MODEL_NAME=$input_model
        
        start_litellm_proxy "$MODEL_NAME"
        export ANTHROPIC_BASE_URL="$LITELLM_HOST"
        export ANTHROPIC_API_KEY="sk-litellm" # Ollama typically doesn't need an API key
        exec claude "$@"
        ;;
    10)
        # Custom
        check_dependencies
        echo -e "${BLUE}Configuring Custom LiteLLM Model...${NC}"
        read -p "Enter LiteLLM Model String: " MODEL_NAME
        read -p "Press Enter to continue..."

        start_litellm_proxy "$MODEL_NAME"
        export ANTHROPIC_BASE_URL="$LITELLM_HOST"
        export ANTHROPIC_API_KEY="sk-litellm"
        exec claude "$@"
        ;;
    *)
        echo "Invalid choice"
        exit 1
        ;;
esac
