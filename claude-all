#!/bin/bash

# Colors
GREEN='\033[0;32m'
BLUE='\033[0;34m'
RED='\033[0;31m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

LITELLM_PORT=8555
LITELLM_HOST="http://127.0.0.1:$LITELLM_PORT"
SCRIPT_DIR="$(dirname "$(realpath "$0")")"

check_dependencies() {
    echo -e "${BLUE}Checking dependencies...${NC}"
    
    if ! command -v npm &> /dev/null; then
        echo -e "${RED}Error: npm is not installed.${NC}"
        exit 1
    fi

    if ! command -v claude &> /dev/null; then
        echo -e "${YELLOW}'claude' command not found. Installing @anthropic-ai/claude-code globally...${NC}"
        npm install -g @anthropic-ai/claude-code
    fi

    if ! command -v python3 &> /dev/null; then
        echo -e "${RED}Error: python3 is not installed.${NC}"
        exit 1
    fi

    if ! python3 -m pip show litellm &> /dev/null; then
        echo -e "${YELLOW}'litellm' not found. Installing via pip...${NC}"
        python3 -m pip install litellm[proxy]
    fi
}

check_gemini_oauth() {
    # Check if we have ADC credentials
    ADC_PATH="$HOME/.config/gcloud/application_default_credentials.json"
    
    if [ -f "$ADC_PATH" ]; then
        return 0
    fi
    
    # If not, check if gcloud is installed
    if command -v gcloud &> /dev/null; then
        echo -e "${BLUE}gcloud found. Attempting login...${NC}"
        gcloud auth application-default login
        return
    fi
    
    # Fallback to custom python script
    echo -e "${YELLOW}gcloud not found. Using lightweight Python Auth helper...${NC}"
    
    # Install dependency
    if ! python3 -m pip show google-auth-oauthlib &> /dev/null; then
         echo "Installing google-auth-oauthlib..."
         python3 -m pip install google-auth-oauthlib
    fi
    
    # Run helper script
    # We expect gemini_auth.py to be in the same dir as this script (if cloned) or downloaded
    AUTH_SCRIPT="$SCRIPT_DIR/gemini_auth.py"
    if [ ! -f "$AUTH_SCRIPT" ]; then
        # If installed via curl, download the helper
        curl -fsSL https://raw.githubusercontent.com/zesbe/CliAllModel/main/gemini_auth.py -o "$HOME/.local/bin/gemini_auth.py"
        AUTH_SCRIPT="$HOME/.local/bin/gemini_auth.py"
    fi
    
    python3 "$AUTH_SCRIPT"
    
    if [ ! -f "$ADC_PATH" ]; then
        echo -e "${RED}Authentication failed or cancelled.${NC}"
        exit 1
    fi
}


start_litellm_proxy() {
    local model=$1
    echo -e "${BLUE}Starting LiteLLM proxy for model: $model...${NC}"
    
    # Kill any existing litellm on this port
    fuser -k $LITELLM_PORT/tcp &> /dev/null || true

    # Start litellm in background
    # --drop_params prevents Anthropic-specific params from breaking other providers
    python3 -m litellm --model "$model" --port $LITELLM_PORT --drop_params &> /tmp/litellm.log &
    LITELLM_PID=$!
    
    # Wait for it to start
    echo -n "Waiting for proxy to start..."
    for i in {1..10}; do
        if curl -s $LITELLM_HOST/health &> /dev/null; then
            echo -e " ${GREEN}Ready!${NC}"
            return 0
        fi
        sleep 1
        echo -n "."
    done
    
    echo -e "\n${RED}Failed to start LiteLLM proxy. Check logs:${NC}"
    cat /tmp/litellm.log
    kill $LITELLM_PID 2>/dev/null
    exit 1
}

cleanup() {
    if [ ! -z "$LITELLM_PID" ]; then
        echo -e "\n${BLUE}Stopping LiteLLM proxy...${NC}"
        kill $LITELLM_PID 2>/dev/null
    fi
}

trap cleanup EXIT

# Main Menu
choice=$1

if [ -z "$choice" ]; then
    clear
    echo -e "${GREEN}=====================================${NC}"
    echo -e "${GREEN}    Claude Code Multi-Model Launcher ${NC}"
    echo -e "${GREEN}=====================================${NC}"
    echo "Select your AI Provider:"
    echo "1) MiniMax (Direct Anthropic API)"
    echo "2) Google Gemini (API Key - AI Studio)"
    echo "3) Google Gemini (OAuth - Vertex AI)"
    echo "4) OpenAI (API Key)"
    echo "5) xAI / Grok (API Key)"
    echo "6) ZhipuAI / GLM (API Key)"
    echo "7) Groq (API Key)"
    echo "8) Ollama (Local Models)"
    echo "9) Custom / Other"
    echo -e "${GREEN}=====================================${NC}"
    read -p "Enter choice [1-9]: " choice
fi

export ANTHROPIC_MODEL="claude-3-sonnet-20240229" # Dummy model name for CLI validation

case $choice in
    1)
        # MiniMax Direct
        echo -e "${BLUE}Configuring for MiniMax...${NC}"
        if [ -z "$ANTHROPIC_API_KEY" ]; then
            echo "Get Key: https://platform.minimax.io/"
            read -p "Enter MiniMax API Key: " ANTHROPIC_API_KEY
            export ANTHROPIC_API_KEY
        fi
        export ANTHROPIC_BASE_URL="https://api.minimax.io/anthropic"
        echo -e "${YELLOW}Note: MiniMax maps model names automatically.${NC}"
        exec claude "$@"
        ;;
    2)
        # Gemini API Key
        check_dependencies
        echo -e "${BLUE}Configuring for Gemini (AI Studio)...${NC}"
        if [ -z "$GEMINI_API_KEY" ]; then
            echo "Get Key: https://aistudio.google.com/app/apikey"
            read -p "Enter Gemini API Key: " GEMINI_API_KEY
            export GEMINI_API_KEY
        fi
        MODEL_NAME="gemini/gemini-1.5-flash" # Use flash by default
        read -p "Enter Model Name [default: $MODEL_NAME]: " input_model
        [ ! -z "$input_model" ] && MODEL_NAME=$input_model
        
        start_litellm_proxy "$MODEL_NAME"
        export ANTHROPIC_BASE_URL="$LITELLM_HOST"
        export ANTHROPIC_API_KEY="sk-litellm"
        exec claude "$@"
        ;;
    3)
        # Gemini Vertex OAuth
        check_dependencies
        echo -e "${BLUE}Configuring for Gemini (Vertex AI OAuth)...${NC}"
        
        check_gemini_oauth

        if [ -z "$VERTEX_PROJECT" ]; then
            read -p "Enter Google Cloud Project ID: " VERTEX_PROJECT
            export VERTEX_PROJECT
        fi
        
        # Vertex specific env setup for LiteLLM
        export VERTEXAI_PROJECT="$VERTEX_PROJECT"
        export VERTEXAI_LOCATION="us-central1"
        
        MODEL_NAME="vertex_ai/gemini-1.5-pro"
        read -p "Enter Model Name [default: $MODEL_NAME]: " input_model
        [ ! -z "$input_model" ] && MODEL_NAME=$input_model
        
        start_litellm_proxy "$MODEL_NAME"
        export ANTHROPIC_BASE_URL="$LITELLM_HOST"
        export ANTHROPIC_API_KEY="sk-litellm"
        exec claude "$@"
        ;;
    4)
        # OpenAI
        check_dependencies
        echo -e "${BLUE}Configuring for OpenAI...${NC}"
        echo -e "${YELLOW}Note: OpenAI does not support OAuth for general CLI API access.${NC}"
        if [ -z "$OPENAI_API_KEY" ]; then
            echo "Get Key: https://platform.openai.com/api-keys"
            read -p "Enter OpenAI API Key: " OPENAI_API_KEY
            export OPENAI_API_KEY
        fi
        MODEL_NAME="gpt-4o"
        read -p "Enter Model Name [default: $MODEL_NAME]: " input_model
        [ ! -z "$input_model" ] && MODEL_NAME=$input_model
        
        start_litellm_proxy "$MODEL_NAME"
        export ANTHROPIC_BASE_URL="$LITELLM_HOST"
        export ANTHROPIC_API_KEY="sk-litellm"
        exec claude "$@"
        ;;
    5)
        # xAI
        check_dependencies
        echo -e "${BLUE}Configuring for xAI (Grok)...${NC}"
        echo -e "${YELLOW}Note: xAI uses API Keys for access.${NC}"
        if [ -z "$XAI_API_KEY" ]; then
            echo "Get Key: https://console.x.ai/"
            read -p "Enter xAI API Key: " XAI_API_KEY
            export XAI_API_KEY
        fi
        MODEL_NAME="xai/grok-1" # Updated to reflect Grok-1
        read -p "Enter Model Name [default: $MODEL_NAME]: " input_model
        [ ! -z "$input_model" ] && MODEL_NAME=$input_model
        
        start_litellm_proxy "$MODEL_NAME"
        export ANTHROPIC_BASE_URL="$LITELLM_HOST"
        export ANTHROPIC_API_KEY="sk-litellm"
        exec claude "$@"
        ;;
    6)
        # ZhipuAI
        check_dependencies
        echo -e "${BLUE}Configuring for ZhipuAI (GLM)...${NC}"
        echo -e "${YELLOW}Note: ZhipuAI uses API Keys for access.${NC}"
        if [ -z "$ZHIPUAI_API_KEY" ]; then
            echo "Get Key: https://open.bigmodel.cn/overview"
            read -p "Enter ZhipuAI API Key: " ZHIPUAI_API_KEY
            export ZHIPUAI_API_KEY
        fi
        MODEL_NAME="zhipu/glm-4"
        read -p "Enter Model Name [default: $MODEL_NAME]: " input_model
        [ ! -z "$input_model" ] && MODEL_NAME=$input_model
        
        start_litellm_proxy "$MODEL_NAME"
        export ANTHROPIC_BASE_URL="$LITELLM_HOST"
        export ANTHROPIC_API_KEY="sk-litellm"
        exec claude "$@"
        ;;
    7)
        # Groq
        check_dependencies
        echo -e "${BLUE}Configuring for Groq...${NC}"
        echo -e "${YELLOW}Note: Groq uses API Keys for access.${NC}"
        if [ -z "$GROQ_API_KEY" ]; then
            echo "Get Key: https://console.groq.com/keys"
            read -p "Enter Groq API Key: " GROQ_API_KEY
            export GROQ_API_KEY
        fi
        MODEL_NAME="groq/llama3-70b-8192"
        read -p "Enter Model Name [default: $MODEL_NAME]: " input_model
        [ ! -z "$input_model" ] && MODEL_NAME=$input_model
        
        start_litellm_proxy "$MODEL_NAME"
        export ANTHROPIC_BASE_URL="$LITELLM_HOST"
        export ANTHROPIC_API_KEY="sk-litellm"
        exec claude "$@"
        ;;
    8)
        # Ollama
        check_dependencies
        echo -e "${BLUE}Configuring for Ollama (Local Models)...${NC}"
        echo -e "${YELLOW}Ensure Ollama server is running (ollama run <model> &).${NC}"
        if [ -z "$OLLAMA_HOST" ]; then
            echo "Ollama Host (e.g., http://localhost:11434): "
            read -p "Enter Ollama Host [default: http://localhost:11434]: " OLLAMA_HOST
            [ -z "$OLLAMA_HOST" ] && OLLAMA_HOST="http://localhost:11434"
            export OLLAMA_HOST
        fi
        MODEL_NAME="ollama/llama3"
        read -p "Enter Model Name [default: $MODEL_NAME]: " input_model
        [ ! -z "$input_model" ] && MODEL_NAME=$input_model
        
        start_litellm_proxy "$MODEL_NAME"
        export ANTHROPIC_BASE_URL="$LITELLM_HOST"
        export ANTHROPIC_API_KEY="sk-litellm" # Ollama typically doesn't need an API key
        exec claude "$@"
        ;;
    9)
        # Custom
        check_dependencies
        echo -e "${BLUE}Configuring Custom LiteLLM Model...${NC}"
        echo "Format example: provider/model-name (e.g., ollama/llama3, azure/gpt-4, cohere/command-r-plus)"
        echo "For Azure, set AZURE_API_KEY, AZURE_API_BASE, AZURE_API_VERSION env vars."
        read -p "Enter LiteLLM Model String: " MODEL_NAME
        
        echo "Set any required env vars (e.g., OLLAMA_HOST, AZURE_API_KEY, OPENROUTER_API_KEY). Press Enter when done."
        read -p "Press Enter to continue..."

        start_litellm_proxy "$MODEL_NAME"
        export ANTHROPIC_BASE_URL="$LITELLM_HOST"
        export ANTHROPIC_API_KEY="sk-litellm"
        exec claude "$@"
        ;;
    *)
        echo "Invalid choice"
        exit 1
        ;;
esac